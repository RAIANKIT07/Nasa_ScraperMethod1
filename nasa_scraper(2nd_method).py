# -*- coding: utf-8 -*-
"""Nasa Scraper(2nd Method).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17CjV6Ozxvhx8AvgyJlJDbo_OJin5svCT

#About the Task
Create a scraper to get data from one of the following websites.
The scrapper file should be in
the .py format and scrapper must have a single python class which will be called to get the required data.

The output should be in the csv format.

Requirements:

● Only pick one of your trial tasks from the sources listed below

Note: This is also a gauge of which type of data structures you are most comfortable with.

● Create scrapper and follow evaluation guidelines below

● Build clean standards, data should contain metadata along with all the values present in the
dataset.

● Simple way to present your data in map, graphs or charts to provide synthesis and show
analytical skills in a short report

# Instaling all required pip
"""

#!pip install requests;
 #!pip install bs4;
 #!pip instal html5lib;

!pip install beautifulsoup4
import os

# Set the PYTHONPATH environment variable
os.environ['PYTHONPATH'] = 'https://www.earthdata.nasa.gov/engage/open-data-services-and-software/api'

import requests
from bs4 import BeautifulSoup
import csv

url1 = "https://www.earthdata.nasa.gov/engage/open-data-services-and-software/api"
url2 = "https://www.nnvl.noaa.gov/view/globaldata.html"
r = requests.get(url1,url2)
htmlcontent = r.content
print (htmlcontent)

# Continue with your web scraping code here using BeautifulSoup

#Parse the HTML
soup = BeautifulSoup(htmlcontent, 'html.parser')
print(soup.prettify)

"""##HTML Tree Traversal"""

#Commonly used types of objects:
print(type(title))   #1.Tag
print(type(title.string))  #2.NavigableString
print(type(soup))  #3.BeautifulSoup



# 4.Comment
markup = "<p><!-- this is a comment --></p>"
soup2 = BeautifulSoup(markup)
print(type(soup2.p))
exist()

#Get the title of HTML page
title = soup.title

#Get all the Anchor tags from the page
paras = soup.find_all('p')
print(paras)

#Get all the paragraph from the page
anchors = soup.find_all('a')
print(anchors)

#Get first element in the HTML page
print(soup.find('p'))

#Get the text from the tags/soup
print(soup.find('p').get_text())
print(soup.get_text())

#Get all the links on the pages:
for link in anchors:
  print(link.get('href'))


  all_links = set()

#Get all the links on the page
for link in anchors:
    href = link.get('href')
    if href is not None and href != '#':
        linkText = "https://www.earthdata.nasa.gov/engage/open-data-services-and-software/api" + href
        all_links.add(linkText)
    print(linkText)  # Note: Make sure to use the correct variable name, which is 'linkText' (not 'linktext').

