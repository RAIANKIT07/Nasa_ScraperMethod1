# -*- coding: utf-8 -*-
"""Bureau of Economic Analysis and Google Data Commons.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11u6D1Pm6sS0MAK5o2tAvkIvxkGi5vwEu

#Task 1:- Bureau of Economic Analysis (BEA)
Python scraper class to get data from the BEA website
"""

import requests
from bs4 import BeautifulSoup
import csv

class BEAScraper:
    def __init__(self):
        self.base_url = 'https://www.bea.gov/'
        self.data = []

    def scrape_data(self):
        try:
            # Send an HTTP GET request to the BEA website
            response = requests.get(self.base_url)
            response.raise_for_status()

            # Parse the HTML content of the page using BeautifulSoup
            soup = BeautifulSoup(response.text, 'html.parser')

            # Find links to different BEA data products (modify as needed)
            data_links = soup.find_all('a', href=True)

            for link in data_links:
                link_text = link.get('href')

                # Filter links to BEA data products (modify as needed)
                if 'bea.gov/data/' in link_text:
                    data_product_name = link.text.strip()
                    data_product_url = link.get('href')
                    self.data.append({
                        'Data Product Name': data_product_name,
                        'Data Product URL': data_product_url
                    })

        except requests.exceptions.RequestException as e:
            print("Error:", e)

    def save_to_csv(self, output_file):
        try:
            with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:
                fieldnames = ['Data Product Name', 'Data Product URL']
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)

                # Write header row
                writer.writeheader()

                # Write data rows
                for item in self.data:
                    writer.writerow(item)

            print(f'Data saved to {output_file}')

        except Exception as e:
            print("Error:", e)

if __name__ == '__main__':
    scraper = BEAScraper()
    scraper.scrape_data()
    scraper.save_to_csv('bea_data_products.csv')

"""#Task 2: Data Commons
Python scraper class to get data from Data Commons:
"""

import requests
from bs4 import BeautifulSoup

class DataCommonsScraper:
    def __init__(self):
        self.base_url = 'https://www.datacommons.org/'
        self.data = []

    def scrape_data(self, page_url):
        try:
            # Send an HTTP GET request to the specified page URL
            response = requests.get(page_url)
            response.raise_for_status()

            # Parse the HTML content of the page using BeautifulSoup
            soup = BeautifulSoup(response.text, 'html.parser')

            # Extract and process the data (modify this part as needed)
            data_elements = soup.find_all('div', class_='data-element')

            for element in data_elements:
                # Extract relevant information from the data element
                data_name = element.find('h2').text.strip()
                data_description = element.find('p').text.strip()

                # Append the extracted data to the list
                self.data.append({
                    'Data Name': data_name,
                    'Data Description': data_description
                })

        except requests.exceptions.RequestException as e:
            print("Error:", e)

    def print_data(self):
        for item in self.data:
            print("Data Name:", item['Data Name'])
            print("Data Description:", item['Data Description'])
            print()

if __name__ == '__main__':
    scraper = DataCommonsScraper()

    # Specify the URL of the Data Commons page you want to scrape
    page_url = 'https://www.datacommons.org/sample-page'

    # Perform the scraping
    scraper.scrape_data('https://www.datacommons.org/sample-page')

    # Print the scraped data
    scraper.print_data()

